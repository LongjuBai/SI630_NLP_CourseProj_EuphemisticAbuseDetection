{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-26 12:44:04,021] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from huggingface_hub import interpreter_login\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/longju/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921fb3454c4a463e935677fbcb6c1676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI assistant designed for in-context learning. I can learn from a few shot examples and generate similar contents. This means that if you provide me with a few examples of a specific topic, style, or tone, I can learn from those examples and generate new content that is similar in nature. I'm here to help you with your queries, provide information, and even assist you in generating content. So, what would you like to know or create?\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are AI assistant designed for in-context learning, you can learn from the few shot examples to generated similar contents\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thanks for asking! I'm a large language model, so I don't have emotions like humans do, but I'm always happy to chat with you and help with any questions or tasks you have. How about you? How's your day going?\n"
     ]
    }
   ],
   "source": [
    "def get_completion(user_prompt):\n",
    "    \n",
    "    input_message = user_prompt\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are AI assistant designed for in-context learning, you can learn from the few shot examples to generated similar contents\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{input_message}\"},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Hey how are you doing today?\"\n",
    "responses = get_completion(prompt)\n",
    "print(responses)\n",
    "\n",
    "\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"gpt-4-turbo-2024-04-09\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "#   ]\n",
    "# )\n",
    "\n",
    "# print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        examples = file.read().strip()\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(csv_input, csv_output, examples_file):\n",
    "    # Load the examples for few-shot learning\n",
    "    examples = load_examples(examples_file)\n",
    "    # Load the CSV file\n",
    "    goldstandard = pd.read_csv(csv_input)\n",
    "\n",
    "\n",
    "    # ------------------------------test\n",
    "    goldstandard = goldstandard.head(5) \n",
    "\n",
    "    \n",
    "    # Initialize the output DataFrame\n",
    "    generated_data = []\n",
    "\n",
    "    # Process each row in the DataFrame\n",
    "    for idx, row in tqdm(goldstandard.iterrows(), total=goldstandard.shape[0], desc=\"Generating Data\"):\n",
    "        prompt = f\"{examples}\\n\\n\" \\\n",
    "                 f\"INSTRUCTION: Write a sentence with the semantic meaning the same as [{row['#CUE_COLUMN']}], \" \\\n",
    "                 f\"use the template [{row['FRAGMENT_COLUMN']}], the sentence should be euphemistic not explicit, \" \\\n",
    "                 f\"the sentence should be [{row['LABEL_COLUMN']}].\" \\\n",
    "                 f\"\\n\\n OUTPUT:\"\n",
    "        generated_sentence = get_completion(prompt)\n",
    "        generated_data.append({\n",
    "            '#CUE_COLUMN': row['#CUE_COLUMN'],\n",
    "            'FRAGMENT_COLUMN': row['FRAGMENT_COLUMN'],\n",
    "            'INSTANCE_COLUMN': generated_sentence,\n",
    "            'LABEL_COLUMN': row['LABEL_COLUMN']\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the generated data\n",
    "    new_df = pd.DataFrame(generated_data)\n",
    "    new_df.to_csv(output_csv, index=False)\n",
    "    print(\"Data generation complete and saved to\", csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = '../modified_goldstandard.csv'\n",
    "output_csv = 'llama_augmented_data_test.csv'\n",
    "# output_csv = 'llama3_augmented_data.csv'\n",
    "examples_file = 'llama_prompt.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:   0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\rGenerating Data:  20%|██        | 1/5 [00:10<00:41, 10.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\rGenerating Data:  40%|████      | 2/5 [00:20<00:30, 10.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\rGenerating Data:  60%|██████    | 3/5 [00:28<00:18,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\rGenerating Data:  80%|████████  | 4/5 [00:38<00:09,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\rGenerating Data: 100%|██████████| 5/5 [00:48<00:00,  9.75s/it]\rGenerating Data: 100%|██████████| 5/5 [00:48<00:00,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generation complete and saved to llama_augmented_data_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_data(input_csv, output_csv, examples_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63a59f273878d21d9c11a68929f8708312fda201dd9668999f076f8cf65bfd79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
