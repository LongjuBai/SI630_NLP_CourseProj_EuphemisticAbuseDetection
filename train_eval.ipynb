{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44690ff2",
   "metadata": {},
   "source": [
    "# Homework 3, Part 2\n",
    "\n",
    "For this part of the homework, we'll work with the data we labeled in Part 1 on estimating human values to fine-tune a deep learning classifier based on a pre-trained language model. Pre-trained language models like BERT or GPT4 have all been trained to do the task of language modeling in some form and their parameters \"know\" about language through doing the task, much like how in Homework 2 the word vectors end up \"knowing\" about word meaning through the task of context word prediction. By starting from these parameters, we can often build a much more effective model for some NLP tasks, such as a classifier. Often, we speak of _fine-tuning_ the parameters for a specific task to distinguish from the _pre-training_ step. Both are training the model, but the former is what we'll do as practitioners to adapt the model to get it to do what we want.\n",
    "\n",
    "To fine-tune our classifier, we'll be using the Huggingface [`transformers`](https://huggingface.co/) library. This library provides many useful functions for working with pre-trained language models and its [model repository](https://huggingface.co/models) contains many pre-trained models that companies and individuals have shared for doing different tasks. With their code, we can often quickly get a basic model up and running.\n",
    "\n",
    "In particular, we'll be going through how to use the Huggingface [`Trainer`](https://huggingface.co/docs/transformers/training) class, which is a powerful, high-level abstraction for how to fine-tune models for many types of different NLP tasks. We'll look at two types of tasks:\n",
    "- Training a regular classifier\n",
    "- Training a _multilabel_ classifier (i.e., one that predicts multiple different labels at the same time)\n",
    "\n",
    "The first type is similar to what we saw in Homeworks 1 and 2. The second type is what we'll need to use with our data annotation where we have zero or more values being present in any given instance ---we want to predict which of them are present at once, not just one at a time.\n",
    "\n",
    "We'll start by going through the basics of how to load data and get it prepared for use with `Trainer` and then extend it to the multilabel case.\n",
    "\n",
    "## Summary of the Training Task and Learning Goals\n",
    "\n",
    "Homework 3 focuses on a subjective annotation/classification task: What values are signaled by an author in their writing? For this part of the homework we've provided example data from the recent SemEval shared task: [SemEval-2023 Task 4: ValueEval: Identification of Human Values Behind Arguments](https://aclanthology.org/2023.semeval-1.313/). This dataset and format will be similar to what we have to annotate in Part 1, so this is just to get you started.\n",
    "\n",
    "Learning goals\n",
    "- Gain experience in working with `torch` and `transformer` libraries \n",
    "- Learn how to tokenize text and create batches\n",
    "- Learn how to fine-tune pre-trained models for different tasks using the `Trainer` class\n",
    "- Learn how to evaluate models\n",
    "- Learn how to train models on a GPU \n",
    "- Learn how to use Great Lakes to submit jobs \n",
    "\n",
    "## How to do this part of the assignment\n",
    "\n",
    "Huggingface makes it easy to fine-tune models. We'll be working with one very small model in this case. To get things started, you can get most of the code working in this notebook, including doing some very small-scale training (e.g., a few training steps) which will verify that all the steps work. Once you get it working, you'll then convert this notebook to a script and run it on Great Lakes to train the whole model and save the model. You only need to use Great Lakes for training.\n",
    "\n",
    "Once you have the final model, you can run the experiments with it on your laptop in the next notebook (Part 3). \n",
    "\n",
    "## Important Notes on Training on Great Lakes\n",
    "\n",
    "This homework requires that you use a GPU for fine-tuning your model. You can use your own or you can use the GPUs on Great Lakes for free.  We have provided you with a course account for Great Lakes, `si630w24`, which will give you access to many hours of GPU time on the cluster using state-of-the-art GPUs. Your Great Lakes jobs are limited to 4 hours of training, which is more than enough to train your model for this assignment. \n",
    "\n",
    "We've put together a [guide](https://docs.google.com/document/d/1YtOkxSGUyX0siaOtKhry-dMgKDqPAycPvlE62wbrBsM/edit?usp=sharing) on how to access Great Lakes, set up your environment, and submit your jobs. We strongly encourage learning to use Great Lakes for this assignment as (1) you have free access to substantial computational resources and (2) you would want to use it for the next homework and the final project.\n",
    "\n",
    "Great Lakes uses a queueing system where users (like you) submit requests for their programs to run (often called \"jobs\"). Great Lakes supports two kinds of job requests: (1) interactive mode jobs that give you a Jupyter notebook and (2) running a script as a job. **We strongly encourage the scripts and discourage interactive jobs for the GPUs.** To get a GPU, you'll need to submit a job to the cluster, which uses [SLURM for scheduling](https://arc.umich.edu/greatlakes/slurm-user-guide/). If you attempt to queue for an interactive job, you will have no control over when it starts, so you may end up having your notebook run for 4 hours from 3am to 7am and then it ends, at which point you have to get back in the queue. If you submit a job as a script (i.e., a .py file that runs the code in this notebook), it will run for the specified amount of time and save your fine-tuned model without you having to interact with anything. \n",
    "\n",
    "SLURM and cluster scheduling are very common in some industries where there is a single cluster resource and people share it by submitting jobs to run so that no one can monopolize the system and that jobs can run in parallel. Given that Great Lakes will be useful to you in future assignments and projects, we strongly encourage you to learn how to use it effectively in this assignment.\n",
    "\n",
    "Depending on how you're working on this file, there are a few ways to directly convert the notebook to a file if you use [Jupyter or the command line](https://mljar.com/blog/convert-jupyter-notebook-python/) or [VSCode](https://stackoverflow.com/questions/64297272/best-way-to-convert-ipynb-to-py-in-vscode). Once you convert it, you'll modify the file some to change the epochs and text file as specified in the PDF. **We also strongly recommend having your script save the model at the end of every epoch.**  That way, if your script takes longer than 4 hours and gets killed, you still have the best-saved model you could get based on the amount of training you could do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7242b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-04 20:47:45,636] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce8ee8",
   "metadata": {},
   "source": [
    "# Task 3.1.1: Load Dataset \n",
    "\n",
    "We'll start by loading the dataset for the [SemEval 2023 Values prediction task](https://aclanthology.org/2023.semeval-1.313/) and converting this into a `Dataset` that we can use to train our model. \n",
    "\n",
    "For the `Trainer` code to work, it expects a column called \"labels\" (unless you want to configure it more heavily). In our case, we've already given you one column to rename as \"labels\". \n",
    "\n",
    "- Start by [loading](https://huggingface.co/docs/datasets/v1.2.1/loading_datasets.html) the training, dev, and test data into a pandas `DataFrame`.\n",
    "- Add or name that column so each of the `DataFrame`s has a \"labels\" column corresponding to the value of the \"Security: personal\" columns\n",
    "- Then convert all of these into a `Dataset`.\n",
    "- Then wrap all of these in [`DatasetDict`]()\n",
    "\n",
    "We recommend reading the [Huggingface documentation](https://huggingface.co/docs/datasets/index) on how to load and interact with datasets, as you'll end up doing this a lot as a practitioner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdbde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See above instructions\n",
    "\n",
    "# Load data into pandas DataFrames\n",
    "train_df = pd.read_csv('si630-w24-train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('si630-w24-dev.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('si630-w24-test.tsv', sep='\\t')\n",
    "\n",
    "# Rename the relevant column to 'labels'\n",
    "train_df = train_df.rename(columns={'Security: personal': 'labels'})\n",
    "dev_df = dev_df.rename(columns={'Security: personal': 'labels'})\n",
    "test_df = test_df.rename(columns={'Security: personal': 'labels'})\n",
    "\n",
    "# Convert DataFrames to Huggingface Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': dev_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3931884f-fa80-4f24-9b6a-b2bedf20cf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inst_id</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01002</td>\n",
       "      <td>We should ban human cloning because as it will...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01005</td>\n",
       "      <td>We should ban fast food because fast food shou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01006</td>\n",
       "      <td>We not should end the use of economic sanction...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01007</td>\n",
       "      <td>We not should abolish capital punishment becau...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01008</td>\n",
       "      <td>We not should ban factory farming because fact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>E08016</td>\n",
       "      <td>The EU should integrate the armed forces of it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>E08017</td>\n",
       "      <td>Food whose production has been subsidized with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>E08018</td>\n",
       "      <td>Food whose production has been subsidized with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5391</th>\n",
       "      <td>E08019</td>\n",
       "      <td>Food whose production has been subsidized with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5392</th>\n",
       "      <td>E08020</td>\n",
       "      <td>The EU should integrate the armed forces of it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5393 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     inst_id                                               text  labels\n",
       "0     A01002  We should ban human cloning because as it will...       0\n",
       "1     A01005  We should ban fast food because fast food shou...       1\n",
       "2     A01006  We not should end the use of economic sanction...       0\n",
       "3     A01007  We not should abolish capital punishment becau...       0\n",
       "4     A01008  We not should ban factory farming because fact...       1\n",
       "...      ...                                                ...     ...\n",
       "5388  E08016  The EU should integrate the armed forces of it...       0\n",
       "5389  E08017  Food whose production has been subsidized with...       1\n",
       "5390  E08018  Food whose production has been subsidized with...       0\n",
       "5391  E08019  Food whose production has been subsidized with...       1\n",
       "5392  E08020  The EU should integrate the armed forces of it...       0\n",
       "\n",
       "[5393 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833baac",
   "metadata": {},
   "source": [
    "# Task 3.2 Preparing the Data\n",
    "\n",
    "Once we have our Dataset created, we need to turn it into features and labels so we can train the model with it.\n",
    "\n",
    "## Task 3.2.1 Tokezing\n",
    "\n",
    "Pre-trained language models are each associated with a specific method for tokenizing data, much like how in Homework 1 you write the `tokenize` function that turns strings into discrete features. Huggingface has a fast [`tokenizers`]() package that we will use here through the [`AutoTokenizer`]() class.\n",
    "\n",
    "On Huggingface, each model is associated with a particular unique string. For example, the original BERT model is `bert-based-uncased`. You can use this string to look up the corresponding tokenizer with `AutoTokenizer`\n",
    "\n",
    "Be sure to set the `model_max_length` argument to indicate what's the maximum length sequence this model can handle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23324e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you can use this smaller model if you want to get started\n",
    "# model_name = 'microsoft/MiniLM-L12-H384-uncased'\n",
    "model_name = \"google-bert/bert-base-cased\"\n",
    "\n",
    "# TODO: Load the tokenizer using AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1c1df",
   "metadata": {},
   "source": [
    "## Task 3.2.2 Converting the labels \n",
    "\n",
    "Each label needs to also be associated with an ID (starting at 0). In 3.2.2, we'll do a _very simple_ pass and **use only one label column** (you'll use more columns later). Your tasks are\n",
    "\n",
    "- Create a list of the labels in the dataset.  For Task 3.2.2, *this list should contains only a single label* (for now)\n",
    "- Create a mapping from ID to label, and the reverse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79a8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label to ID: {0: 0, 1: 1}\n",
      "ID to Label: {0: 0, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "labels = train_df['labels'].unique()\n",
    "id2label = label2id = {label: id for id, label in enumerate(labels)}\n",
    "label2id = id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "\n",
    "print(\"Label to ID:\", label2id)\n",
    "print(\"ID to Label:\", id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3e340",
   "metadata": {},
   "source": [
    "## Task 3.2.3: Preprocessing the data\n",
    "\n",
    "Models are typically fine-tuned using _batches_ rather than single instances. For the attention-based classifier in Homework 2 Part 2, we used a batch size of 1 because of the key challenge in batching over sequences: texts have different lengths, but all items in a batch need to be the same length. \n",
    "\n",
    "Here, we'll use a function to _create_ batches of the same length by padding them with extra tokens typically labeled as `[PAD]`. The underlying model code knows to avoid doing computation with these tokens so they don't have any effect on the text, other than making the tensors all the same size; if you're curious, look around for details of the \"attention mask\" to see how the \"[PAD]\" token gets ignored (you'll also see this in the tokenized dataset object too!).\n",
    "\n",
    "One of the big reasons we use a `Dataset` is that it supports easy preprocessing to turn the text into IDs and do this truncation for us. We'll define the function below that says how to transform the instances and then call `map` on the dataset to get the preprocessed/tokenized data back out.\n",
    "\n",
    "**Important Note**: One key hyperparameter to deal with is the maximum length of the sequence. If you recall from our attention, the attention mechanism is $O(n^2)$ for the length $n$ of a sequence. This means long sequences get very expensive in the kinds of models we'll use here. Since all the items in a batch get padded to the length of either the longest sequence in a batch or the maximum length of the model, one long sequence can mean we use a lot more memory just to hold empty [PAD] tokens. As a result, often we can truncate very long sequences to make them fit in memory, under the assumption that the extra tokens aren't that informative. In the code below, we use the longest sequence in a batch which hopefully helps keep things smaller. However, in your projects, you may explore other truncation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892bb4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc46c331ce1f46d19600b730d2211079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21bd7d995da43d3a625b33ba7b76111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1896 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b7e8edf2934c4596452a8b76e9e89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples, padding=\"longest\", truncation=True):\n",
    "\n",
    "    ### TODO: Tokenize the text using the tokenizer using the \n",
    "    # specified function arguments\n",
    "    # pass\n",
    "    return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True)\n",
    "    \n",
    "# TODO: Call .map on the dataset to tokenize the data\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a75e5f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inst_id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5393\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['inst_id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1896\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inst_id', 'text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1576\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what all got added to the tokenized_datasets\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f422315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] We should ban human cloning because as it will only cause huge issues when you have a bunch of the same humans running around all acting the same. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at what we have\n",
    "example = tokenized_datasets['train'][0]\n",
    "\n",
    "# We can reverse the tokenization to see the original text too\n",
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3869687",
   "metadata": {},
   "source": [
    "### Decide on where the computation will be\n",
    "\n",
    "The deep learning library under all the huggingface code is `torch` which is a set of libraries for doing matrix math quickly. Different hardware have different capabilities for how to do math so torch lets you specify where you want to do the computation with the `device` argument. GPUs are designed to do fast matrix multiplication and so are ideal for our purposes. As you might have noticed in homework 2, `torch` can change the device though and will run just fine (but slower) on the GPU. The code snippet below shows how to choose the device.\n",
    "\n",
    "**Important Note:** When doing the computation both the data and the parameters need to be on the same device. This means if you move the pre-trained model to the GPU but the data is sitting on the CPU, the model can't see the data (or vice-versa). This is why you'll see a lot of `something.to(device)` to everything is on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58631a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "device = 'cpu' \n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f83a3",
   "metadata": {},
   "source": [
    "## Task 3.3.1: Getting the model and `Trainer` setup\n",
    "\n",
    "Now it's time to bring in the model. Just like with the tokenizer, we can use huggingface's `AutoModelFor[TASK_TYPE]` to load the model for the appropriate task type. In this case, we want to classify a sequence of tokens, so we'll use the `AutoModelForSequenceClassification` class but there are many other options after \"For\" that you can check out (e.g., for your course project)\n",
    "\n",
    "In this case, we need to specify how many classes/labels there are in our data.\n",
    "Calculate that from the data above and provide it as an argument when loading the parameters for the pre-trained model we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3835e120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load Pre-trained model from HuggingFace Model Hub\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6448a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 108311810 || all params: 108311810 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "## Let's see how many parameters we are going to be changing\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cff9e9",
   "metadata": {},
   "source": [
    "## Task 3.3.2 Setting up the training arguments\n",
    "\n",
    "There are a lot of options when you fine-tune a model! In homework 2, you saw a few of these when we set the learning rate and number of epochs. When using the huggingface `Trainer` class, we specify all of the training arguments at once in a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments) object. There are a _lot_ of arguments you can specify and for the most part, you do not need to set all of them. Reading that class's documentation can be overwhelming, so don't worry if you don't know what all of them mean.\n",
    "\n",
    "Here are a few useful arguments that you'll need to set:\n",
    "- `output_dir` - where to save the models. This directory can get very large if you save all the checkpoints!\n",
    "- `overwrite_output_dir` - whether to overwrite the previously saved models\n",
    "- `learning_rate` - use 2e-5,\n",
    "- `per_device_train_batch_size` - how many items per batch. You usually want this to be a power of 2 due to how computers work. Common sizes with GPUs are between 64 and 256, but it will depend a lot on how much memory the GPU has (and how big your sequences are). \n",
    "- `per_device_eval_batch_size` - same as above, but because you're not doing gradient descent on these (just eval), there's less \"stuff\" needed in memory and this can be a bit larger.\n",
    "- `do_eval` - whether do evaluate on the development/validation data periodically during training\n",
    "- `seed` - the random seed to use. Use 12345\n",
    "- `evaluation_strategy` - when to evaluate the model during training. \"epoch\" evaluates after the end of every epoch, while \"steps\" evaluates every `eval_steps`. If you have a very large dataset, you probably want to use \"steps\" so that you can get periodic updates on how the model is doing. Even though our dataset is relatively small, let's use \"steps\".\n",
    "- `eval_steps` - how many steps between an evaluation on the dev data. For this assignment, use 50 so we can see how our model trains. In real-world scenarios, you'll often have this larger (e.g., 1000) so that you're not spending more of your GPU time evaluating instead of training\n",
    "- `save_strategy` - how over to save your model's parameters during training. This is either \"epoch\" or \"steps\", where `save_steps` is used. The logic for setting the argument is similar to that for `evaluation_strategy`. Because our dataset is relatively small, use \"steps\".\n",
    "- `save_steps` - how many steps between saving the model's parameters. Typically you set this the same as `eval_steps` which we'll do here.\n",
    "- `num_train_epochs` - how many epochs to train for. Use 10 for now.\n",
    "- `logging_dir` - where to save the log files.\n",
    "- `load_best_model_at_end` -  Whether or not to load the best model found during training at the end of training. When this option is enabled, the best checkpoint will always be saved. This is kind of a sneakily-important argument. If you set this to `True`, the `Trainer` will automatically keep track of what the best model is so far (checked at every `save_strategy`) so you always have a copy of the parameters on disk in the checkpoint file for the best version. If you don't set this, you might keep training and never save your best model! Set this to `True`.\n",
    "- `metric_for_best_model` - another important argument: this says how we should define our \"best\" model in terms of a metric. We could use loss on the training data with \"loss\", but since we have an evaluation dataset, we'll choose based on performance on that model. When looking at the metrics on the dev/evaluation/validation dataset, all of the metrics get prefixed with \"eval_\". In this case, use \"eval_f1\"\n",
    "- `greater_is_better` - if we're setting `metric_for_best_model` we need to tell the `Trainer` which direction is better, e.g., lower is better for \"loss\" but greater is better for metrics like \"f1\". \n",
    "- `report_to` - the `Trainer` code is hooked into common logging libraries. We'll use `wandb` like in Homework 2. You might not even need to do anything for it to log but you'll need to make sure you can get plots showing up on Weights & Biases for the homework.\n",
    "\n",
    "Here are a few useful arguments you won't need here but you might want to try or explore later:\n",
    "- `fp16` - Most floating-point computation is done with 32 bits. However, some modern GPUs and even CPUs can support floating-point operations with fewer bits. These operations are faster, though less precise. Because of the sheer number of calculations, it's often useful to prioritize speed and you can turn on 16-bit floating point by setting `fp16=True`. There are a bunch of other options like this if you're curious (good office hours discussion too).\n",
    "- `gradient_accumulation_steps` - For complex NLP tasks, sometimes we only have enough GPU memory for very small batches (e.g., 2 or 4 items). However, we can simulate big batches by asking the trainer to _accumulate_ (i.e., sum) the gradients across batches before taking the update step, which allows us to have arbitrarily large \"accumulated batches\". \n",
    "- `lr_scheduler_type` - In our previous homework, our SGD/AdamW optimizers all used the same learning rate at each step. But our model gets better every step, so sometimes we might want to be able to make smaller updates so we don't change too much and make the model worse. This argument lets you choose different learning rate [schedulers](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/optimizer_schedules#transformers.SchedulerType) that will dynamically change the learning rate based on how training is going.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a049398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./logs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=eval_f1,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/nfs/turbo/coe-mihalcea/longju/SI630HW3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/nfs/turbo/coe-mihalcea/longju/SI630HW3,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=12345,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# NOTE: when debugging the evaluation code, feel free to turn down the eval_steps\n",
    "# to a small number so that training evaluates right away.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/nfs/turbo/coe-mihalcea/longju/SI630HW3\",  # Directory for saving models\n",
    "    overwrite_output_dir=True,  # Overwrite the content of the output directory\n",
    "    learning_rate=2e-5,  # Learning rate\n",
    "    per_device_train_batch_size=64,  # Batch size for training\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation\n",
    "    do_eval=True,  # Perform evaluation during training\n",
    "    seed=12345,  # Random seed\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every `eval_steps`\n",
    "    eval_steps=50,  # Number of steps to run evaluation\n",
    "    save_strategy=\"steps\",  # Save the model every `save_steps`\n",
    "    save_steps=50,  # Number of steps to save the model\n",
    "    num_train_epochs=10,  # Number of training epochs\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    "    metric_for_best_model=\"eval_f1\",  # Use eval_f1 to evaluate the best model\n",
    "    greater_is_better=True,  # Higher eval_f1 indicates a better model\n",
    "    report_to=\"wandb\"  # Use Weights & Biases for logging\n",
    ")\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a60e34",
   "metadata": {},
   "source": [
    "## Task 3.3.3 Defining some evaluation metrics\n",
    "\n",
    "How good is our model? We'll need to provide the `Trainer` some function that given some predictions, can evaluate how good the predictions are. When the `Trainer` instance calls this function, it will pass in a tuple that contains the logits of the model's predictions. These are the unnormalized weights for each of the labels (the logit is the inverse function of the sigmoid). We _could_ normalize these back with a softmax, but instead, we can simply figure out which dimension's value is largest and say that's the dimension label.\n",
    "\n",
    "The `compute_metrics` function can return a dictionary that maps a metric name to its value. This will let us track multiple metrics over time. All of these metrics also get recorded with `wandb` by `Trainer` too so we'll see how the model trains. **Important Note:** When the `Trainer` class evaluates on the development data, the key names for the matrics get prefixed with \"eval_\", so if we reported a dictionary with \"f1\" as a key, we'd see a corresponding metric of \"eval_f1\" in our logs.\n",
    "\n",
    "For our model, we'll compute binary F1. This only keeps track of the positive class, which is appropriate in our case where we want to know whether the model is good at finding a model in terms of its precision and recall. Use the `sklearn` to calculate these.\n",
    "\n",
    "**NOTE:** Earlier we had to convert all the labels/classes into IDs starting from 0 and this code helps explain why--each of the classes has its own dimension!\n",
    "\n",
    "**NOTE:** The labels we get in the `eval_pred` tuple are the same labels that we specified in our dataset. `Trainer` looks for this column so that it can pass it through the `AutoModel` and have it reported here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6687835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric to use for evaluation\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    # TODO: \n",
    "    # 1. Get the logits and labels from the eval_pred\n",
    "    # 2. Compute the predictions from the logits\n",
    "    # 3. Calculate binary precision, recal, and F1\n",
    "    # 4. Return the values as a dictionary with key names for\n",
    "    #    indicating the metric\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391c0bf",
   "metadata": {},
   "source": [
    "## Task 3.3.4: Setup the `Trainer`\n",
    "\n",
    "Finally! Let's specify the `Trainer` that's going to run the training. Most of our arguments and hyperparameters have already been specified in the `TrainingArguments` but there are still a few things we need to specify:\n",
    "\n",
    "- `model` - A pre-trained model to fine-tune\n",
    "- `args` - the `TrainingArguments` we just defined\n",
    "- `train_dataset` - the training portion of the tokenized dataset\n",
    "- `eval_dataset` - the portion of the tokenized dataset that we'll use during training to evaluate \n",
    "- `tokenizer` - the tokenizer model used to turn text into sequences\n",
    "- `compute_metrics` - the `compute_metrics` function we just defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd262185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the Trainer object's arguments\n",
    "trainer = Trainer(\n",
    "    model=model,  # The pre-trained model\n",
    "    args=training_args,  # The TrainingArguments we defined earlier\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # The training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],  # The evaluation dataset\n",
    "    tokenizer=tokenizer,  # The tokenizer\n",
    "    compute_metrics=compute_metrics  # The metric function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3bacaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlongju\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/longju/Homework 3/wandb/run-20240404_192146-wbu0ewwi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/longju/huggingface/runs/wbu0ewwi' target=\"_blank\">fast-dawn-2</a></strong> to <a href='https://wandb.ai/longju/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/longju/huggingface' target=\"_blank\">https://wandb.ai/longju/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/longju/huggingface/runs/wbu0ewwi' target=\"_blank\">https://wandb.ai/longju/huggingface/runs/wbu0ewwi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='850' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [850/850 08:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562582</td>\n",
       "      <td>0.617391</td>\n",
       "      <td>0.748353</td>\n",
       "      <td>0.676593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.507099</td>\n",
       "      <td>0.731672</td>\n",
       "      <td>0.657444</td>\n",
       "      <td>0.692575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.482514</td>\n",
       "      <td>0.691228</td>\n",
       "      <td>0.778656</td>\n",
       "      <td>0.732342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.506046</td>\n",
       "      <td>0.683223</td>\n",
       "      <td>0.815547</td>\n",
       "      <td>0.743544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.493626</td>\n",
       "      <td>0.710828</td>\n",
       "      <td>0.735178</td>\n",
       "      <td>0.722798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.579007</td>\n",
       "      <td>0.772370</td>\n",
       "      <td>0.648221</td>\n",
       "      <td>0.704871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.600184</td>\n",
       "      <td>0.679669</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.716511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658995</td>\n",
       "      <td>0.765203</td>\n",
       "      <td>0.596838</td>\n",
       "      <td>0.670614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.794205</td>\n",
       "      <td>0.745033</td>\n",
       "      <td>0.592885</td>\n",
       "      <td>0.660308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.804946</td>\n",
       "      <td>0.740219</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.676681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.808766</td>\n",
       "      <td>0.720567</td>\n",
       "      <td>0.669302</td>\n",
       "      <td>0.693989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.867908</td>\n",
       "      <td>0.731523</td>\n",
       "      <td>0.638999</td>\n",
       "      <td>0.682138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.939320</td>\n",
       "      <td>0.744300</td>\n",
       "      <td>0.602108</td>\n",
       "      <td>0.665696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.965121</td>\n",
       "      <td>0.761658</td>\n",
       "      <td>0.581028</td>\n",
       "      <td>0.659193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.955342</td>\n",
       "      <td>0.739683</td>\n",
       "      <td>0.613966</td>\n",
       "      <td>0.670986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.989045</td>\n",
       "      <td>0.722307</td>\n",
       "      <td>0.627141</td>\n",
       "      <td>0.671368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>1.011464</td>\n",
       "      <td>0.737520</td>\n",
       "      <td>0.603426</td>\n",
       "      <td>0.663768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=850, training_loss=0.20993914211497589, metrics={'train_runtime': 523.1698, 'train_samples_per_second': 103.083, 'train_steps_per_second': 1.625, 'total_flos': 4794525789634200.0, 'train_loss': 0.20993914211497589, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's train!\n",
    "trainer.train() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b47be60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.506045937538147, 'eval_precision': 0.6832229580573952, 'eval_recall': 0.8155467720685112, 'eval_f1': 0.7435435435435436, 'eval_runtime': 3.1623, 'eval_samples_per_second': 599.562, 'eval_steps_per_second': 9.487, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Once we finish training, we can evaluate the model on the dev set. Note that\n",
    "# since we specified the trainer to load the best model at the end, the \n",
    "# trainer will automatically load the best model for us to use here.\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c004f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the trainer to predict() on the test set and then score the predictions\n",
    "test_preds = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62e8125c-03c0-40a8-b4d0-1f24269a3949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.33226264, -0.76825434],\n",
       "       [ 1.518177  , -2.2027586 ],\n",
       "       [-2.2347767 ,  0.9374685 ],\n",
       "       ...,\n",
       "       [-0.23518634, -0.8393739 ],\n",
       "       [ 1.370341  , -1.9539418 ],\n",
       "       [-1.5006099 ,  0.8468643 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 0, 0, 1]), metrics={'test_loss': 0.46613338589668274, 'test_precision': 0.6413199426111909, 'test_recall': 0.8324022346368715, 'test_f1': 0.7244732576985412, 'test_runtime': 2.693, 'test_samples_per_second': 585.219, 'test_steps_per_second': 9.283})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "63a59f273878d21d9c11a68929f8708312fda201dd9668999f076f8cf65bfd79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
